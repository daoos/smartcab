{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teach a smartcab to drive\n",
    "## Tasks\n",
    "\n",
    "## Setup\n",
    "\n",
    "You need Python 2.7 and pygame for this project: https://www.pygame.org/wiki/GettingStarted\n",
    "For help with installation, it is best to reach out to the pygame community [help page, Google group, reddit].\n",
    "\n",
    "## Download\n",
    "\n",
    "Download smartcab.zip, unzip and open the template Python file agent.py (do not modify any other file). Perform the following tasks to build your agent, referring to instructions mentioned in README.md as well as inline comments in agent.py.\n",
    "\n",
    "Also create a project report (e.g. Word or Google doc), and start addressing the questions indicated in italics below. When you have finished the project, save/download the report as a PDF and turn it in with your code.\n",
    "\n",
    "## Implement a basic driving agent\n",
    "\n",
    "Implement the basic driving agent, which processes the following inputs at each time step:\n",
    "\n",
    "Next waypoint location, relative to its current location and heading,\n",
    "Intersection state (traffic light and presence of cars), and,\n",
    "Current deadline value (time steps remaining),\n",
    "And produces some random move/action (None, 'forward', 'left', 'right'). Don’t try to implement the correct strategy! That’s exactly what your agent is supposed to learn.\n",
    "\n",
    "Run this agent within the simulation environment with enforce_deadline set to False (see run function in agent.py), and observe how it performs. In this mode, the agent is given unlimited time to reach the destination. The current state, action taken by your agent and reward/penalty earned are shown in the simulator.\n",
    "\n",
    "In your report, mention what you see in the agent’s behavior. Does it eventually make it to the target location?\n",
    "\n",
    "## Identify and update state\n",
    "\n",
    "Identify a set of states that you think are appropriate for modeling the driving agent. The main source of state variables are current inputs, but not all of them may be worth representing. Also, you can choose to explicitly define states, or use some combination (vector) of inputs as an implicit state.\n",
    "\n",
    "At each time step, process the inputs and update the current state. Run it again (and as often as you need) to observe how the reported state changes through the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulator.run(): Trial 0\n",
      "Environment.reset(): Trial set up with start = (8, 2), destination = (2, 3), deadline = 35\n",
      "RoutePlanner.route_to(): destination = (2, 3)\n",
      "----------------------------------------------------------\n",
      "RESET, Final state:\n",
      "None\n",
      "----------------------------------------------------------\n",
      "LearningAgent.update(): self.state(35, {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'forward', None, None, 0), action = forward, reward = -1.0, next_waypoint = forward\n",
      "LearningAgent.update(): self.state(34, {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'forward', 'forward', 'forward', -1.0), action = forward, reward = -1.0, next_waypoint = forward\n",
      "LearningAgent.update(): self.state(33, {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'forward', 'forward', 'forward', -1.0), action = forward, reward = -1.0, next_waypoint = forward\n",
      "LearningAgent.update(): self.state(32, {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'forward', 'forward', 'forward', -1.0), action = forward, reward = 2.0, next_waypoint = forward\n",
      "LearningAgent.update(): self.state(31, {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'forward', 'forward', 'forward', 2.0), action = forward, reward = -1.0, next_waypoint = forward\n",
      "LearningAgent.update(): self.state(30, {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'forward', 'forward', 'forward', -1.0), action = right, reward = -0.5, next_waypoint = forward\n",
      "LearningAgent.update(): self.state(29, {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'left', 'right', 'forward', -0.5), action = None, reward = 0.0, next_waypoint = left\n",
      "LearningAgent.update(): self.state(28, {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'left', None, 'left', 0.0), action = left, reward = -1.0, next_waypoint = left\n",
      "LearningAgent.update(): self.state(27, {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'left', 'left', 'left', -1.0), action = forward, reward = -1.0, next_waypoint = left\n",
      "LearningAgent.update(): self.state(26, {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'left', 'forward', 'left', -1.0), action = left, reward = 2.0, next_waypoint = left\n",
      "LearningAgent.update(): self.state(25, {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'forward', 'left', 'left', 2.0), action = forward, reward = -1.0, next_waypoint = forward\n",
      "LearningAgent.update(): self.state(24, {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'forward', 'forward', 'forward', -1.0), action = right, reward = -0.5, next_waypoint = forward\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import string\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import random\n",
    "from environment import Agent, Environment\n",
    "from planner import RoutePlanner\n",
    "from simulator import Simulator\n",
    "       \n",
    "\n",
    "\n",
    "sys.path.append(\"./smartcab/\")\n",
    "import agent \n",
    "def run():\n",
    "    \"\"\"Run the agent for a finite number of trials.\"\"\"\n",
    "\n",
    "    # Set up environment and agent\n",
    "    e = Environment()  # create environment (also adds some dummy traffic)\n",
    "    a = e.create_agent(agent.LearningAgent)  # create agent\n",
    "    e.set_primary_agent(a, enforce_deadline=False)  # specify agent to track\n",
    "    # NOTE: You can set enforce_deadline=False while debugging to allow longer trials\n",
    "\n",
    "    # Now simulate it\n",
    "    sim = Simulator(e, update_delay=0.5, display=True)  # create simulator (uses pygame when display=True, if available)\n",
    "    # NOTE: To speed up simulation, reduce update_delay and/or set display=False\n",
    "\n",
    "    sim.run(n_trials=100)  # run for a specified number of trials\n",
    "    # NOTE: To quit midway, press Esc or close pygame window, or hit Ctrl+C on the command-line\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Q-Learning\n",
    "\n",
    "Implement the Q-Learning algorithm by initializing and updating a table/mapping of Q-values at each time step. Now, instead of randomly selecting an action, pick the best action available from the current state based on Q-values, and return that.\n",
    "\n",
    "Each action generates a corresponding numeric reward or penalty (which may be zero). Your agent should take this into account when updating Q-values. Run it again, and observe the behavior.\n",
    "\n",
    "What changes do you notice in the agent’s behavior?\n",
    "\n",
    "## Enhance the driving agent\n",
    "\n",
    "Apply the reinforcement learning techniques you have learnt, and tweak the parameters (e.g. learning rate, discount factor, action selection method, etc.), to improve the performance of your agent. Your goal is to get it to a point so that within 100 trials, the agent is able to learn a feasible policy - i.e. reach the destination within the allotted time, with net reward remaining positive.\n",
    "\n",
    "Report what changes you made to your basic implementation of Q-Learning to achieve the final version of the agent. How well does it perform?\n",
    "\n",
    "Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties?\n",
    "\n",
    " PREVIOUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#eof"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
