{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teach a smartcab to drive\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "You need Python 2.7 and pygame for this project: https://www.pygame.org/wiki/GettingStarted\n",
    "For help with installation, it is best to reach out to the pygame community [help page, Google group, reddit].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready\n"
     ]
    }
   ],
   "source": [
    "# Import what we need, and setup the basic function to run from later.\n",
    "\n",
    "import math\n",
    "import string\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "# Show matplotlib plots inline (nicely formatted in the notebook)\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append(\"./smartcab/\")\n",
    "from environment import Agent, Environment\n",
    "from planner import RoutePlanner\n",
    "from simulator import Simulator\n",
    "\n",
    "print \"Environment ready\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test a\n",
      "redirecting stdout....\n",
      "stdout restored!\n",
      "redirecting stdout....\n",
      "stdout restored!\n",
      "Redirector ready\n"
     ]
    }
   ],
   "source": [
    "class outputRedirect():\n",
    "    def __init__(self):\n",
    "        import sys\n",
    "        import os\n",
    "        self.stout_orig=sys.stdout \n",
    "    \n",
    "    def reset(self):\n",
    "        sys.stdout = self.stout_orig\n",
    "        print \"stdout restored!\"\n",
    "        \n",
    "    def suppress_output(self):\n",
    "        self.redirect_output()\n",
    "        \n",
    "    def redirect_output(self,f= open(os.devnull, 'w')):\n",
    "        try:\n",
    "            print \"redirecting stdout....\"\n",
    "            sys.stdout = f\n",
    "        except:\n",
    "            return \"couldn't open passed\"\n",
    "            self.reset = f\n",
    "            \n",
    "redirector=outputRedirect()\n",
    "\n",
    "print \"test a\"\n",
    "redirector.suppress_output()\n",
    "print \"test b\"\n",
    "redirector.reset()\n",
    "redirector.redirect_output(open(os.devnull, 'w'))\n",
    "print \"test c\"\n",
    "redirector.reset()\n",
    "print \"Redirector ready\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def run(agentType,trials=10, gui=False, deadline=False, delay=0):\n",
    "    \"\"\"Run the agent for a finite number of trials.\"\"\"\n",
    "    \n",
    "    # Set up environment and agent\n",
    "    e = Environment()  # create environment (also adds some dummy traffic)\n",
    "    a = e.create_agent(agentType)  # create agent\n",
    "    e.set_primary_agent(a, enforce_deadline=deadline)  # specify agent to track\n",
    "    # NOTE: You can set enforce_deadline=False while debugging to allow longer trials\n",
    "\n",
    "    # Now simulate it\n",
    "    sim = Simulator(e, update_delay=delay, display=gui)  # create simulator (uses pygame when display=True, if available)\n",
    "    # NOTE: To speed up simulation, reduce update_delay and/or set display=False\n",
    "\n",
    "    sim.run(n_trials=trials)  # run for a specified number of trials\n",
    "    # NOTE: To quit midway, press Esc or close pygame window, or hit Ctrl+C on the command-line\n",
    "    print \"Successfull runs = {}\".format(a.goal)\n",
    "    print \"----------------------------------------------------------\"\n",
    "    features= []\n",
    "    deadlines = []\n",
    "    for i in range(len(a.features)):\n",
    "        features.append(pd.DataFrame(a.features[i]).T)\n",
    "        deadlines.append(a.deadline[i])\n",
    "        \n",
    "    rewards=[]\n",
    "    for i in range(len(a.total_reward)):\n",
    "        rewards.append(a.total_reward[i])\n",
    "\n",
    "\n",
    "    try:\n",
    "        print \"Qtable:\"\n",
    "        for r in a.Qtable:\n",
    "            print r, a.Qtable[r]\n",
    "    except:\n",
    "        print \"no Qtable\"\n",
    "\n",
    "    return features,deadlines,rewards\n",
    "\n",
    "print \"run ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a basic driving agent\n",
    "\n",
    "Implement the basic driving agent, which processes the following inputs at each time step:\n",
    "\n",
    "Next waypoint location, relative to its current location and heading,\n",
    "Intersection state (traffic light and presence of cars), and,\n",
    "Current deadline value (time steps remaining),\n",
    "And produces some random move/action (None, 'forward', 'left', 'right'). Don’t try to implement the correct strategy! That’s exactly what your agent is supposed to learn.\n",
    "\n",
    "Run this agent within the simulation environment with enforce_deadline set to False (see run function in agent.py), and observe how it performs. In this mode, the agent is given unlimited time to reach the destination. The current state, action taken by your agent and reward/penalty earned are shown in the simulator.\n",
    "\n",
    "In your report, mention what you see in the agent’s behavior. Does it eventually make it to the target location?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    \"\"\"An agent that learns to drive in the smartcab world.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(RandomAgent, self).__init__(env)  # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'red'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        # TODO: Initialize any additional variables here\n",
    "        self.availableAction = [None, 'forward', 'left', 'right']   \n",
    "        self.goal=0\n",
    "        self.steps=0\n",
    "        self.features=[]\n",
    "        self.deadline=[]\n",
    "        self.total_reward=[0]\n",
    "\n",
    "    def reset(self, destination=None):\n",
    "        self.planner.route_to(destination)\n",
    "        # TODO: Prepare for a new trip; reset any variables here, if required\n",
    "        #print\"RESET, Final state:\\n\", self.state\n",
    "        try:\n",
    "            if self.deadline[len(self.features)-1] >0: #deadline less than zero\n",
    "                self.goal+=1 #FIXME - order\n",
    "                print \"PASS! {} steps to goal,Goal reached {} times out of {}!\".format(self.deadline[len(self.features)-1],self.goal,len(self.features))\n",
    "            else:\n",
    "                print \"FAIL! {} steps to goal,Goal reached {} times out of {}!\".format(self.deadline[len(self.features)-1],self.goal,len(self.features))\n",
    "                pass\n",
    "        except:\n",
    "            print \"Trial 0 - Goal reached {} times out of {}!\".format(self.goal,len(self.features))\n",
    "            pass\n",
    "        print \"----------------------------------------------------------\"\n",
    "        self.features.append({})\n",
    "        self.deadline.append(None)\n",
    "        self.total_reward.append(0)\n",
    "        self.steps=0\n",
    "\n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.steps+=1\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        #self.deadline[len(self.features)] = self.env.get_deadline(self)\n",
    "        self.state=inputs\n",
    "        self.features[len(self.features)-1][self.steps]=inputs\n",
    "        self.deadline[len(self.deadline)-1] = self.env.get_deadline(self)\n",
    "\n",
    "        # TODO: Select action according to your policy\n",
    "        action = self.availableAction[random.randint(0,3)]    \n",
    "        \n",
    "        # Execute action and get reward\n",
    "        reward = self.env.act(self, action)\n",
    "        self.lastReward=reward\n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "        self.total_reward[len(self.total_reward)-1] =self.total_reward[len(self.total_reward)-1]+reward\n",
    "        #print \"LearningAgent.update():deadline{}, inputs{}, action = {}, reward = {}, next_waypoint = {}\".format(\n",
    "        #                                            deadline, inputs, action, reward,self.next_waypoint, )  # [debug]\n",
    "print \"RandomAgent ready\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features,deadlines, rewards=run(agentType=RandomAgent,trials=2, deadline=False) #Example of a random run, with no deadline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features,deadlines, rewards=run(agentType=RandomAgent,trials=2, deadline=True) #Example of a random run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent - Discussion:\n",
    "\n",
    "When we run an agent with a random action policy, we see that it will move about the board with no pattern, and will eventually reach the destination. If we allow the use of deadlines, we see that the agent rarely reaches the destination in time, although it may still occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Identify and update state\n",
    "\n",
    "Identify a set of states that you think are appropriate for modeling the driving agent. The main source of state variables are current inputs, but not all of them may be worth representing. Also, you can choose to explicitly define states, or use some combination (vector) of inputs as an implicit state.\n",
    "\n",
    "At each time step, process the inputs and update the current state. Run it again (and as often as you need) to observe how the reported state changes through the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class StateAgent(RandomAgent):\n",
    "    \"\"\"An agent that learns to drive in the smartcab world.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(StateAgent, self).__init__(env)  # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'red'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        # TODO: Initialize any additional variables here\n",
    "        self.availableAction = [None, 'forward', 'left', 'right']   \n",
    "        self.next_waypoint   = None\n",
    "        self.goal=0\n",
    "        self.steps=0\n",
    "        self.features=[]\n",
    "        self.total_reward=[0]\n",
    "\n",
    "        \n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.steps+=1\n",
    "        \n",
    "        self.lastWaypoint = self.next_waypoint\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        \n",
    "        deadline = self.env.get_deadline(self)\n",
    "        \n",
    "        # TODO: Update state\n",
    "        \n",
    "        inputs['next_waypoint']=self.next_waypoint\n",
    "        self.state= inputs    \n",
    "        self.deadline[len(self.deadline)-1] = self.env.get_deadline(self)\n",
    "        self.features[len(self.features)-1][self.steps]=inputs\n",
    "        # TODO: Select action according to your policy\n",
    "\n",
    "        action = self.availableAction[random.randint(0,3)]    \n",
    "    \n",
    "        # Execute action and get reward\n",
    "        reward = self.env.act(self, action)\n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "\n",
    "        self.total_reward[len(self.total_reward)-1] =self.total_reward[len(self.total_reward)-1]+reward\n",
    "\n",
    "        #print \"LearningAgent.update(): self.state{}, action = {}, reward = {}, next_waypoint = {}\".format(\n",
    "        #                                        self.state, action, reward,self.next_waypoint, )  # [debug]\n",
    "print \"StateAgent Ready\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the trials for the state\n",
    "\n",
    "# turn off output to stdout, as the chatter  from the various pieces is too muuch info\n",
    "saveout = sys.stdout  \n",
    "f = open(os.devnull, 'w')\n",
    "sys.stdout = f\n",
    "\n",
    "stateFeatures,StateDeadlines,StateRewards=run(agentType=StateAgent,trials=25)\n",
    "\n",
    "sys.stdout = saveout     # output ack on\n",
    "\n",
    "print \"Random Agent done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display the feedback from the prior run\n",
    "def statsFromRun(stateFeatures):\n",
    "    left=pd.Series()\n",
    "    light=pd.Series()\n",
    "    next_waypoint=pd.Series()\n",
    "    oncoming=pd.Series()\n",
    "    right=pd.Series()\n",
    "    for f in stateFeatures:\n",
    "        left= left.add(pd.value_counts(f.left.ravel()), fill_value=0)\n",
    "        light= light.add(pd.value_counts(f.light.ravel()), fill_value=0)\n",
    "        next_waypoint= next_waypoint.add(pd.value_counts(f.next_waypoint.ravel()), fill_value=0)\n",
    "        oncoming= oncoming.add(pd.value_counts(f.oncoming.ravel()), fill_value=0)\n",
    "        right= right.add(pd.value_counts(f.right.ravel()), fill_value=0)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3,figsize=(14,6))\n",
    "    fig.suptitle( \"Runs:{}\".format(len(stateFeatures)))\n",
    "\n",
    "    left.plot(kind='bar', title=\"Left\",ax=axes[0,0])\n",
    "    light.plot(kind='bar', title=\"light\",ax=axes[0,1])\n",
    "    next_waypoint.plot(kind='bar', title=\"next_waypoint\",ax=axes[0,2])\n",
    "    oncoming.plot(kind='bar', title=\"oncoming\",ax=axes[1,0])\n",
    "    right.plot(kind='bar', title=\"right\",ax=axes[1,2])\n",
    "    fig.show()\n",
    "    \n",
    "statsFromRun(stateFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scorePerRun(DL,RW):\n",
    "    plt.plot(DL,label=\"Deadlines\")\n",
    "    plt.plot(RW,label=\"Rewards\")\n",
    "    plt.xlabel('Run')\n",
    "    plt.legend()\n",
    "    plt.title(\"Deadline and Rewards per Run\")\n",
    "    plt.show()\n",
    "    \n",
    "scorePerRun(StateDeadlines,StateRewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and update state - Discussion.\n",
    "When we sense our environment, we perceive 5 variables, with several possible states These include: left, light, next_waypoint, oncoming, and right. We can see right away that light and next_waypoint contains new information at every poll, while the others usually have no value. \n",
    "\n",
    "It's not readily apparent that the direction of travel information of the other cars (described by left/right/oncoming) is relevant to our agent. A case could be made to remove the direction information, and only retain information about another car being present at the light. This would have the benefit of reducing the number of possible states, increasing the speed of the agent. This may be a valuable approach in resource constrained environments. \n",
    "\n",
    "The downside is that the agent may pick an action that causes a longer trip. Early in the learning phase, it could also pick an action incorrectly. For instance, by proceeding through a light when the opposite car is turning left. In this case, it may have previously seen a positive reward for moving through the light, because the opposite car was not turning. This time through, it will recieve a negative reward, and in the future when a car is at the oncoming light, it will always wait till the intersection is clear.\n",
    "\n",
    "In the interest of correctness, we will choose to use the state as returned from the sensor, with the addition of the next_waypoint.\n",
    "\n",
    "While I have tracked the deadline, it is not apparent that it will provide useful information to the agent. It is useful to see note that the agent does not see any usefull increase in the deadline value yet. We may expect this to adapt as we implement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Q-Learning\n",
    "\n",
    "Implement the Q-Learning algorithm by initializing and updating a table/mapping of Q-values at each time step. Now, instead of randomly selecting an action, pick the best action available from the current state based on Q-values, and return that.\n",
    "\n",
    "Each action generates a corresponding numeric reward or penalty (which may be zero). Your agent should take this into account when updating Q-values. Run it again, and observe the behavior.\n",
    "\n",
    "What changes do you notice in the agent’s behavior?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BasicLearningAgent(RandomAgent):\n",
    "    \"\"\"An agent that learns to drive in the smartcab world.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(BasicLearningAgent, self).__init__(env)  # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'red'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        # TODO: Initialize any additional variables here\n",
    "        self.availableAction = [None, 'forward', 'left', 'right']   \n",
    "        self.next_waypoint   = None\n",
    "        self.goal=0\n",
    "        self.steps=0\n",
    "        self.features=[]\n",
    "        self.Qtable={}\n",
    "        self.epsilon=0.1\n",
    "        self.gamma=0\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.total_reward=[0]\n",
    "\n",
    "        \n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.steps+=1\n",
    "        \n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        \n",
    "        deadline = self.env.get_deadline(self)\n",
    "        # TODO: Update state\n",
    "        \n",
    "        inputs['next_waypoint']=self.next_waypoint\n",
    "        self.state = (inputs['light'], inputs['oncoming'], inputs['right'], inputs['left'],inputs['next_waypoint'])\n",
    "        self.deadline[len(self.deadline)-1] = self.env.get_deadline(self)\n",
    "        self.features[len(self.features)-1][self.steps]=inputs\n",
    "        # TODO: Select action according to your policy\n",
    "\n",
    "        action = self.availableAction[random.randint(0,3)]    #take a random action\n",
    "        \n",
    "        # 1-epsilon % of time, refer to the q-table for an action. take the max value from the available actions\n",
    "        if self.epsilon < random.random() and  self.Qtable.has_key(self.state): \n",
    "            action=self.availableAction[self.Qtable[self.state].index(max(self.Qtable[self.state]))]\n",
    "                                                    \n",
    "        # Execute action and get reward\n",
    "        reward = self.env.act(self, action)\n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "        if self.Qtable.has_key(self.state):\n",
    "            self.Qtable[self.state][self.availableAction.index(action)]=reward\n",
    "        else:\n",
    "            self.Qtable[self.state]=[0,0,0,0]\n",
    "            self.Qtable[self.state][self.availableAction.index(action)]=reward\n",
    "        \n",
    "        self.last_state = self.state\n",
    "        self.last_action = action\n",
    "        self.total_reward[len(self.total_reward)-1] =self.total_reward[len(self.total_reward)-1]+reward\n",
    "\n",
    "        #print \"LearningAgent.update(): self.state{}, action = {}, reward = {}, next_waypoint = {}\".format(\n",
    "        #                                        self.state, action, reward,self.next_waypoint, )  # [debug]\n",
    "print \"BasicLearningAgent Ready\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run the trials for the Basic Q learning agent\n",
    "\n",
    "# turn off output to stdout, as the chatter  from the various pieces is too muuch info\n",
    "saveout = sys.stdout  \n",
    "f = open(os.devnull, 'w')\n",
    "sys.stdout = f\n",
    "\n",
    "basicLearnFeatures,BLdeadlines,BLrewards=run(agentType=BasicLearningAgent,trials=100, deadline=True) \n",
    "\n",
    "sys.stdout = saveout  # turn output back on\n",
    "\n",
    "print \"Basic Q Learning Agent done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statsFromRun(basicLearnFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scorePerRun(BLdeadlines,BLrewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Q-Learning - Discussion\n",
    "With a basic Qlearning algorithm, we note that the agent quickly learns a set of rules that allow the agent to move toward the objective. Generarlly speaking the agent, is moving to the destination, but may not always make optimal choices. With this version of Q-Learning, we don't see a large continued increase in the agent speed toward the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhance the driving agent\n",
    "\n",
    "Apply the reinforcement learning techniques you have learnt, and tweak the parameters (e.g. learning rate, discount factor, action selection method, etc.), to improve the performance of your agent. Your goal is to get it to a point so that within 100 trials, the agent is able to learn a feasible policy - i.e. reach the destination within the allotted time, with net reward remaining positive.\n",
    "\n",
    "Report what changes you made to your basic implementation of Q-Learning to achieve the final version of the agent. How well does it perform?\n",
    "\n",
    "Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhance the driving agent - Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print  \"running....\"\n",
    "    basicLearnFeatures,BLdeadlines=run(agentType=BasicLearningAgent,trials=50, gui=True, delay=.5)\n",
    "    plt.plot(BLdeadlines,label=\"Deadlines\")\n",
    "    plt.plot(BLrewards,label=\"Rewards\")\n",
    "    plt.xlabel('Run')\n",
    "    plt.legend()\n",
    "    plt.title(\"Deadline and Rewards per Run\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
